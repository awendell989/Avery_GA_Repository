{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn import preprocessing, neighbors, grid_search, cross_validation\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/averyw/Desktop/GA/DS-SF-32/lessons/lesson-8/dataset-boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    BLACK  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston dataset concerns itself with housing values in suburbs of Boston.  A description of the dataset is as follows:\n",
    "\n",
    "- CRIM: per capita crime rate by town\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sqft\n",
    "- INDUS: proportion of non-retail business acres per town\n",
    "- CHAS: Charles River binary/dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX: nitric oxides concentration (parts per 10 million)\n",
    "- RM: average number of rooms per dwelling\n",
    "- AGE: proportion of owner-occupied units built prior to 1940\n",
    "- DIS: weighted distances to five Boston employment centers\n",
    "- RAD: index of accessibility to radial highways\n",
    "- TAX: full-value property-tax rate (per ten thousands of dollars)\n",
    "- PTRATIO: pupil-teacher ratio by town\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT: % lower status of the population\n",
    "- MEDV: Median value of owner-occupied homes (in thousands of dollars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.  \n",
    "+ Let's first categorize `MEDV` to 4 groups: Bottom 20% as Level 1, next 30% as Level 2, next 30% categorized as Level 3, and the top 20% as Level 4.  \n",
    "+ Please create a new variable `MEDV_Category` that stores the level number\n",
    "+ Remember the quantile function\n",
    "+ Remember how to segment your pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Level_3\n",
      "1      Level_2\n",
      "2      Level_3\n",
      "3      Level_3\n",
      "4      Level_4\n",
      "5      Level_3\n",
      "6      Level_3\n",
      "7      Level_3\n",
      "8      Level_1\n",
      "9      Level_2\n",
      "10     Level_1\n",
      "11     Level_2\n",
      "12     Level_2\n",
      "13     Level_2\n",
      "14     Level_1\n",
      "15     Level_2\n",
      "16     Level_3\n",
      "17     Level_1\n",
      "18     Level_2\n",
      "19     Level_1\n",
      "20     Level_1\n",
      "21     Level_2\n",
      "22     Level_1\n",
      "23     Level_1\n",
      "24     Level_1\n",
      "25     Level_1\n",
      "26     Level_1\n",
      "27     Level_1\n",
      "28     Level_2\n",
      "29     Level_2\n",
      "30     Level_1\n",
      "31     Level_1\n",
      "32     Level_1\n",
      "33     Level_1\n",
      "34     Level_1\n",
      "35     Level_2\n",
      "36     Level_2\n",
      "37     Level_2\n",
      "38     Level_3\n",
      "39     Level_3\n",
      "40     Level_4\n",
      "41     Level_3\n",
      "42     Level_3\n",
      "43     Level_3\n",
      "44     Level_2\n",
      "45     Level_2\n",
      "46     Level_2\n",
      "47     Level_1\n",
      "48     Level_1\n",
      "49     Level_2\n",
      "        ...   \n",
      "456    Level_1\n",
      "457    Level_1\n",
      "458    Level_1\n",
      "459    Level_2\n",
      "460    Level_1\n",
      "461    Level_1\n",
      "462    Level_2\n",
      "463    Level_2\n",
      "464    Level_2\n",
      "465    Level_2\n",
      "466    Level_2\n",
      "467    Level_2\n",
      "468    Level_2\n",
      "469    Level_2\n",
      "470    Level_2\n",
      "471    Level_2\n",
      "472    Level_3\n",
      "473    Level_3\n",
      "474    Level_1\n",
      "475    Level_1\n",
      "476    Level_1\n",
      "477    Level_1\n",
      "478    Level_1\n",
      "479    Level_2\n",
      "480    Level_3\n",
      "481    Level_3\n",
      "482    Level_3\n",
      "483    Level_2\n",
      "484    Level_2\n",
      "485    Level_2\n",
      "486    Level_2\n",
      "487    Level_2\n",
      "488    Level_1\n",
      "489    Level_1\n",
      "490    Level_1\n",
      "491    Level_1\n",
      "492    Level_2\n",
      "493    Level_2\n",
      "494    Level_3\n",
      "495    Level_3\n",
      "496    Level_2\n",
      "497    Level_2\n",
      "498    Level_2\n",
      "499    Level_1\n",
      "500    Level_1\n",
      "501    Level_2\n",
      "502    Level_2\n",
      "503    Level_3\n",
      "504    Level_2\n",
      "505    Level_1\n",
      "Name: MEDV_Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "df['MEDV_Category'] = 0.0\n",
    "#np.linspace(0,1,10)\n",
    "df['MEDV_Category'] = pd.qcut(df['MEDV'],10,labels= False)/10.0\n",
    "df['MEDV_Category'] = np.where(df['MEDV_Category'] <= .2,'Level_1',np.where(df['MEDV_Category'] <= .5,'Level_2',np.where(df['MEDV_Category'] <= .8,'Level_3','Level_4')))\n",
    "print df['MEDV_Category']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal is to predict `MEDV_Category` based on `RM`, `PTRATIO`, and `LSTAT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.  \n",
    "\n",
    "+ First normalize `RM`, `PTRATIO`, and `LSTAT`.  \n",
    "+ By normalizing, we mean to scale each variable between 0 and 1 with the lowest value as 0 and the highest value as 1\n",
    "\n",
    "+ Check out the documentation for MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.089680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.204470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.063466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.033389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.099338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.096026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.469630</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.295254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.500287</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.480684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.396628</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.778146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.468097</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.424117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.539567</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.516556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.469055</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.318433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.446062</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.385762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.457559</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.180188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.485725</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.235375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.185982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.454876</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.133830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.465415</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.357064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.363096</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.274834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.415022</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.263521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.384940</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.532285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.460625</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.333885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.468819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.500828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.452769</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.402042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.390496</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.407837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.360927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.476336</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.429084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.562177</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.305464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.596474</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.282837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.412340</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.575883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.481127</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.312086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.457751</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.716887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.410040</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.458609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.485725</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.513521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.454493</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.219371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.436865</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.267108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.438590</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.194260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.460816</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.231788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.581337</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.071468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.663537</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.614869</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.085817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.499713</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.112583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.507760</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.157561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.480552</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.215784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.233996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.426327</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.342715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.473079</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.471026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.352175</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.802428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.391071</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.399283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.462732</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.476821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.455068</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.419702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.525005</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.400110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0.482851</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.357892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.601648</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.405353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.539375</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.356512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.528071</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.565626</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.236203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.507377</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.317053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.421153</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.342163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.425497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.467906</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.540563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.453152</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.452539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.412340</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.359547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.499329</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.401766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.307395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.551063</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.348510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.274007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.357540</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.452815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.498371</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.617274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.560069</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.467715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.333972</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.639625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.449779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.314018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.513700</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.248620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.611037</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.165839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.670627</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.145695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.421728</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.239790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.442614</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.320364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.527112</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.244205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.489174</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.365618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.449128</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.268212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.362713</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.450607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.355049</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.613687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.293543</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.771247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.464074</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.450883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.464074</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.320640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.411190</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.283664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.453152</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.327263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.437914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.350450</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.535596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.427860</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.341336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.470971</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.308775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.384748</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.368929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.472504</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.347682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.219095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.202815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.654340</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.107892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.131071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.473079</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.169702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RM   PTRATIO     LSTAT\n",
       "0    0.577505  0.287234  0.089680\n",
       "1    0.547998  0.553191  0.204470\n",
       "2    0.694386  0.553191  0.063466\n",
       "3    0.658555  0.648936  0.033389\n",
       "4    0.687105  0.648936  0.099338\n",
       "5    0.549722  0.648936  0.096026\n",
       "6    0.469630  0.276596  0.295254\n",
       "7    0.500287  0.276596  0.480684\n",
       "8    0.396628  0.276596  0.778146\n",
       "9    0.468097  0.276596  0.424117\n",
       "10   0.539567  0.276596  0.516556\n",
       "11   0.469055  0.276596  0.318433\n",
       "12   0.446062  0.276596  0.385762\n",
       "13   0.457559  0.893617  0.180188\n",
       "14   0.485725  0.893617  0.235375\n",
       "15   0.435524  0.893617  0.185982\n",
       "16   0.454876  0.893617  0.133830\n",
       "17   0.465415  0.893617  0.357064\n",
       "18   0.363096  0.893617  0.274834\n",
       "19   0.415022  0.893617  0.263521\n",
       "20   0.384940  0.893617  0.532285\n",
       "21   0.460625  0.893617  0.333885\n",
       "22   0.494539  0.893617  0.468819\n",
       "23   0.431500  0.893617  0.500828\n",
       "24   0.452769  0.893617  0.402042\n",
       "25   0.390496  0.893617  0.407837\n",
       "26   0.431500  0.893617  0.360927\n",
       "27   0.476336  0.893617  0.429084\n",
       "28   0.562177  0.893617  0.305464\n",
       "29   0.596474  0.893617  0.282837\n",
       "30   0.412340  0.893617  0.575883\n",
       "31   0.481127  0.893617  0.312086\n",
       "32   0.457751  0.893617  0.716887\n",
       "33   0.410040  0.893617  0.458609\n",
       "34   0.485725  0.893617  0.513521\n",
       "35   0.454493  0.702128  0.219371\n",
       "36   0.436865  0.702128  0.267108\n",
       "37   0.438590  0.702128  0.194260\n",
       "38   0.460816  0.702128  0.231788\n",
       "39   0.581337  0.606383  0.071468\n",
       "40   0.663537  0.606383  0.006898\n",
       "41   0.614869  0.563830  0.085817\n",
       "42   0.499713  0.563830  0.112583\n",
       "43   0.507760  0.563830  0.157561\n",
       "44   0.480552  0.563830  0.215784\n",
       "45   0.406400  0.563830  0.233996\n",
       "46   0.426327  0.563830  0.342715\n",
       "47   0.473079  0.563830  0.471026\n",
       "48   0.352175  0.563830  0.802428\n",
       "49   0.391071  0.563830  0.399283\n",
       "..        ...       ...       ...\n",
       "456  0.462732  0.808511  0.476821\n",
       "457  0.455068  0.808511  0.419702\n",
       "458  0.525005  0.808511  0.400110\n",
       "459  0.482851  0.808511  0.357892\n",
       "460  0.601648  0.808511  0.405353\n",
       "461  0.539375  0.808511  0.356512\n",
       "462  0.528071  0.808511  0.338300\n",
       "463  0.565626  0.808511  0.236203\n",
       "464  0.507377  0.808511  0.317053\n",
       "465  0.421153  0.808511  0.342163\n",
       "466  0.458134  0.808511  0.425497\n",
       "467  0.467906  0.808511  0.540563\n",
       "468  0.453152  0.808511  0.452539\n",
       "469  0.412340  0.808511  0.359547\n",
       "470  0.499329  0.808511  0.401766\n",
       "471  0.511209  0.808511  0.307395\n",
       "472  0.551063  0.808511  0.348510\n",
       "473  0.655106  0.808511  0.274007\n",
       "474  0.357540  0.808511  0.452815\n",
       "475  0.498371  0.808511  0.617274\n",
       "476  0.560069  0.808511  0.467715\n",
       "477  0.333972  0.808511  0.639625\n",
       "478  0.502778  0.808511  0.449779\n",
       "479  0.511209  0.808511  0.314018\n",
       "480  0.513700  0.808511  0.248620\n",
       "481  0.611037  0.808511  0.165839\n",
       "482  0.670627  0.808511  0.145695\n",
       "483  0.421728  0.808511  0.239790\n",
       "484  0.442614  0.808511  0.320364\n",
       "485  0.527112  0.808511  0.244205\n",
       "486  0.489174  0.808511  0.365618\n",
       "487  0.449128  0.808511  0.268212\n",
       "488  0.362713  0.797872  0.450607\n",
       "489  0.355049  0.797872  0.613687\n",
       "490  0.293543  0.797872  0.771247\n",
       "491  0.464074  0.797872  0.450883\n",
       "492  0.464074  0.797872  0.320640\n",
       "493  0.411190  0.702128  0.283664\n",
       "494  0.453152  0.702128  0.327263\n",
       "495  0.404100  0.702128  0.437914\n",
       "496  0.350450  0.702128  0.535596\n",
       "497  0.427860  0.702128  0.341336\n",
       "498  0.470971  0.702128  0.308775\n",
       "499  0.384748  0.702128  0.368929\n",
       "500  0.472504  0.702128  0.347682\n",
       "501  0.580954  0.893617  0.219095\n",
       "502  0.490324  0.893617  0.202815\n",
       "503  0.654340  0.893617  0.107892\n",
       "504  0.619467  0.893617  0.131071\n",
       "505  0.473079  0.893617  0.169702\n",
       "\n",
       "[506 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "a = df[['RM','PTRATIO','LSTAT']]\n",
    "df[['RM','PTRATIO','LSTAT']] =  preprocessing.MinMaxScaler().fit_transform(a)\n",
    "df[['RM','PTRATIO','LSTAT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.  \n",
    "\n",
    "+ Run a k-NN classifier with 5 nearest neighbors and report your misclassification error; set weights to uniform\n",
    "+ Calculate your misclassification error on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "x = df[['RM', 'PTRATIO','LSTAT']]\n",
    "y = df['MEDV_Category']\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn.fit(x,y)\n",
    "#print knn.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211462450593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn.predict(x)\n",
    "y\n",
    "misc_error = 1- accuracy_score(knn.predict(x),y)\n",
    "\n",
    "print misc_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. \n",
    "+ Is this error reliable? \n",
    "+ What could we do to make it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: No its not. \n",
    "\n",
    "First of all I am testing on the same data I am tarining on. \n",
    "We could use cross-validation to make it better. Second of all, just using one k-fold is not wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.  \n",
    "+ Now use 10-fold cross-validation to choose the most efficient `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.62451, std: 0.10655, params: {'n_neighbors': 1, 'weights': 'uniform'},\n",
       " mean: 0.62451, std: 0.10655, params: {'n_neighbors': 1, 'weights': 'distance'},\n",
       " mean: 0.62846, std: 0.11088, params: {'n_neighbors': 2, 'weights': 'uniform'},\n",
       " mean: 0.62451, std: 0.10655, params: {'n_neighbors': 2, 'weights': 'distance'},\n",
       " mean: 0.64032, std: 0.09467, params: {'n_neighbors': 3, 'weights': 'uniform'},\n",
       " mean: 0.63241, std: 0.10256, params: {'n_neighbors': 3, 'weights': 'distance'},\n",
       " mean: 0.66601, std: 0.08208, params: {'n_neighbors': 4, 'weights': 'uniform'},\n",
       " mean: 0.65810, std: 0.08393, params: {'n_neighbors': 4, 'weights': 'distance'},\n",
       " mean: 0.68182, std: 0.09279, params: {'n_neighbors': 5, 'weights': 'uniform'},\n",
       " mean: 0.65613, std: 0.10061, params: {'n_neighbors': 5, 'weights': 'distance'},\n",
       " mean: 0.70158, std: 0.08583, params: {'n_neighbors': 6, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.09831, params: {'n_neighbors': 6, 'weights': 'distance'},\n",
       " mean: 0.71146, std: 0.08819, params: {'n_neighbors': 7, 'weights': 'uniform'},\n",
       " mean: 0.68379, std: 0.08904, params: {'n_neighbors': 7, 'weights': 'distance'},\n",
       " mean: 0.69170, std: 0.09292, params: {'n_neighbors': 8, 'weights': 'uniform'},\n",
       " mean: 0.69565, std: 0.09359, params: {'n_neighbors': 8, 'weights': 'distance'},\n",
       " mean: 0.70949, std: 0.10049, params: {'n_neighbors': 9, 'weights': 'uniform'},\n",
       " mean: 0.70751, std: 0.09661, params: {'n_neighbors': 9, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.09926, params: {'n_neighbors': 10, 'weights': 'uniform'},\n",
       " mean: 0.71739, std: 0.09647, params: {'n_neighbors': 10, 'weights': 'distance'},\n",
       " mean: 0.71146, std: 0.08845, params: {'n_neighbors': 11, 'weights': 'uniform'},\n",
       " mean: 0.71937, std: 0.08726, params: {'n_neighbors': 11, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.08064, params: {'n_neighbors': 12, 'weights': 'uniform'},\n",
       " mean: 0.72332, std: 0.09002, params: {'n_neighbors': 12, 'weights': 'distance'},\n",
       " mean: 0.71344, std: 0.07732, params: {'n_neighbors': 13, 'weights': 'uniform'},\n",
       " mean: 0.71344, std: 0.07557, params: {'n_neighbors': 13, 'weights': 'distance'},\n",
       " mean: 0.70949, std: 0.08371, params: {'n_neighbors': 14, 'weights': 'uniform'},\n",
       " mean: 0.72134, std: 0.08543, params: {'n_neighbors': 14, 'weights': 'distance'},\n",
       " mean: 0.71937, std: 0.07980, params: {'n_neighbors': 15, 'weights': 'uniform'},\n",
       " mean: 0.71739, std: 0.07248, params: {'n_neighbors': 15, 'weights': 'distance'},\n",
       " mean: 0.72727, std: 0.08419, params: {'n_neighbors': 16, 'weights': 'uniform'},\n",
       " mean: 0.72530, std: 0.08857, params: {'n_neighbors': 16, 'weights': 'distance'},\n",
       " mean: 0.71542, std: 0.08006, params: {'n_neighbors': 17, 'weights': 'uniform'},\n",
       " mean: 0.72530, std: 0.08176, params: {'n_neighbors': 17, 'weights': 'distance'},\n",
       " mean: 0.71542, std: 0.08525, params: {'n_neighbors': 18, 'weights': 'uniform'},\n",
       " mean: 0.71937, std: 0.08618, params: {'n_neighbors': 18, 'weights': 'distance'},\n",
       " mean: 0.70751, std: 0.07028, params: {'n_neighbors': 19, 'weights': 'uniform'},\n",
       " mean: 0.71937, std: 0.08462, params: {'n_neighbors': 19, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.08323, params: {'n_neighbors': 20, 'weights': 'uniform'},\n",
       " mean: 0.71739, std: 0.08844, params: {'n_neighbors': 20, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.08346, params: {'n_neighbors': 21, 'weights': 'uniform'},\n",
       " mean: 0.72134, std: 0.08451, params: {'n_neighbors': 21, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.08234, params: {'n_neighbors': 22, 'weights': 'uniform'},\n",
       " mean: 0.71937, std: 0.08662, params: {'n_neighbors': 22, 'weights': 'distance'},\n",
       " mean: 0.70553, std: 0.07500, params: {'n_neighbors': 23, 'weights': 'uniform'},\n",
       " mean: 0.71937, std: 0.08462, params: {'n_neighbors': 23, 'weights': 'distance'},\n",
       " mean: 0.69960, std: 0.07128, params: {'n_neighbors': 24, 'weights': 'uniform'},\n",
       " mean: 0.70949, std: 0.08192, params: {'n_neighbors': 24, 'weights': 'distance'},\n",
       " mean: 0.69368, std: 0.07553, params: {'n_neighbors': 25, 'weights': 'uniform'},\n",
       " mean: 0.71146, std: 0.07896, params: {'n_neighbors': 25, 'weights': 'distance'},\n",
       " mean: 0.68972, std: 0.07406, params: {'n_neighbors': 26, 'weights': 'uniform'},\n",
       " mean: 0.70553, std: 0.08058, params: {'n_neighbors': 26, 'weights': 'distance'},\n",
       " mean: 0.68775, std: 0.07814, params: {'n_neighbors': 27, 'weights': 'uniform'},\n",
       " mean: 0.71344, std: 0.07736, params: {'n_neighbors': 27, 'weights': 'distance'},\n",
       " mean: 0.67984, std: 0.08977, params: {'n_neighbors': 28, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07837, params: {'n_neighbors': 28, 'weights': 'distance'},\n",
       " mean: 0.68182, std: 0.08023, params: {'n_neighbors': 29, 'weights': 'uniform'},\n",
       " mean: 0.70949, std: 0.07877, params: {'n_neighbors': 29, 'weights': 'distance'},\n",
       " mean: 0.67984, std: 0.07840, params: {'n_neighbors': 30, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07858, params: {'n_neighbors': 30, 'weights': 'distance'},\n",
       " mean: 0.68577, std: 0.08207, params: {'n_neighbors': 31, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.07647, params: {'n_neighbors': 31, 'weights': 'distance'},\n",
       " mean: 0.67984, std: 0.08082, params: {'n_neighbors': 32, 'weights': 'uniform'},\n",
       " mean: 0.69763, std: 0.07961, params: {'n_neighbors': 32, 'weights': 'distance'},\n",
       " mean: 0.68775, std: 0.08478, params: {'n_neighbors': 33, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.08003, params: {'n_neighbors': 33, 'weights': 'distance'},\n",
       " mean: 0.68379, std: 0.07423, params: {'n_neighbors': 34, 'weights': 'uniform'},\n",
       " mean: 0.70158, std: 0.07875, params: {'n_neighbors': 34, 'weights': 'distance'},\n",
       " mean: 0.68775, std: 0.07778, params: {'n_neighbors': 35, 'weights': 'uniform'},\n",
       " mean: 0.70158, std: 0.07735, params: {'n_neighbors': 35, 'weights': 'distance'},\n",
       " mean: 0.68182, std: 0.07594, params: {'n_neighbors': 36, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.07393, params: {'n_neighbors': 36, 'weights': 'distance'},\n",
       " mean: 0.67589, std: 0.07462, params: {'n_neighbors': 37, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07342, params: {'n_neighbors': 37, 'weights': 'distance'},\n",
       " mean: 0.66798, std: 0.07705, params: {'n_neighbors': 38, 'weights': 'uniform'},\n",
       " mean: 0.70158, std: 0.07201, params: {'n_neighbors': 38, 'weights': 'distance'},\n",
       " mean: 0.67391, std: 0.07988, params: {'n_neighbors': 39, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07429, params: {'n_neighbors': 39, 'weights': 'distance'},\n",
       " mean: 0.67194, std: 0.08070, params: {'n_neighbors': 40, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.07286, params: {'n_neighbors': 40, 'weights': 'distance'},\n",
       " mean: 0.67391, std: 0.07637, params: {'n_neighbors': 41, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07599, params: {'n_neighbors': 41, 'weights': 'distance'},\n",
       " mean: 0.67391, std: 0.07637, params: {'n_neighbors': 42, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.06244, params: {'n_neighbors': 42, 'weights': 'distance'},\n",
       " mean: 0.67391, std: 0.07682, params: {'n_neighbors': 43, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07564, params: {'n_neighbors': 43, 'weights': 'distance'},\n",
       " mean: 0.67589, std: 0.08804, params: {'n_neighbors': 44, 'weights': 'uniform'},\n",
       " mean: 0.70751, std: 0.07813, params: {'n_neighbors': 44, 'weights': 'distance'},\n",
       " mean: 0.66601, std: 0.09425, params: {'n_neighbors': 45, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07564, params: {'n_neighbors': 45, 'weights': 'distance'},\n",
       " mean: 0.67194, std: 0.08795, params: {'n_neighbors': 46, 'weights': 'uniform'},\n",
       " mean: 0.70553, std: 0.07491, params: {'n_neighbors': 46, 'weights': 'distance'},\n",
       " mean: 0.66601, std: 0.09151, params: {'n_neighbors': 47, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07688, params: {'n_neighbors': 47, 'weights': 'distance'},\n",
       " mean: 0.66403, std: 0.09125, params: {'n_neighbors': 48, 'weights': 'uniform'},\n",
       " mean: 0.70356, std: 0.07564, params: {'n_neighbors': 48, 'weights': 'distance'},\n",
       " mean: 0.66403, std: 0.09618, params: {'n_neighbors': 49, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.07343, params: {'n_neighbors': 49, 'weights': 'distance'},\n",
       " mean: 0.66403, std: 0.09087, params: {'n_neighbors': 50, 'weights': 'uniform'},\n",
       " mean: 0.70158, std: 0.07582, params: {'n_neighbors': 50, 'weights': 'distance'},\n",
       " mean: 0.65415, std: 0.09782, params: {'n_neighbors': 51, 'weights': 'uniform'},\n",
       " mean: 0.69960, std: 0.07617, params: {'n_neighbors': 51, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.09981, params: {'n_neighbors': 52, 'weights': 'uniform'},\n",
       " mean: 0.69763, std: 0.07624, params: {'n_neighbors': 52, 'weights': 'distance'},\n",
       " mean: 0.65415, std: 0.09739, params: {'n_neighbors': 53, 'weights': 'uniform'},\n",
       " mean: 0.69763, std: 0.08344, params: {'n_neighbors': 53, 'weights': 'distance'},\n",
       " mean: 0.65415, std: 0.09832, params: {'n_neighbors': 54, 'weights': 'uniform'},\n",
       " mean: 0.69565, std: 0.07921, params: {'n_neighbors': 54, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10073, params: {'n_neighbors': 55, 'weights': 'uniform'},\n",
       " mean: 0.69565, std: 0.07321, params: {'n_neighbors': 55, 'weights': 'distance'},\n",
       " mean: 0.66008, std: 0.10234, params: {'n_neighbors': 56, 'weights': 'uniform'},\n",
       " mean: 0.69763, std: 0.07266, params: {'n_neighbors': 56, 'weights': 'distance'},\n",
       " mean: 0.65613, std: 0.10623, params: {'n_neighbors': 57, 'weights': 'uniform'},\n",
       " mean: 0.69368, std: 0.07266, params: {'n_neighbors': 57, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.10148, params: {'n_neighbors': 58, 'weights': 'uniform'},\n",
       " mean: 0.69170, std: 0.07466, params: {'n_neighbors': 58, 'weights': 'distance'},\n",
       " mean: 0.65415, std: 0.10426, params: {'n_neighbors': 59, 'weights': 'uniform'},\n",
       " mean: 0.69368, std: 0.07423, params: {'n_neighbors': 59, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10247, params: {'n_neighbors': 60, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07462, params: {'n_neighbors': 60, 'weights': 'distance'},\n",
       " mean: 0.65415, std: 0.10240, params: {'n_neighbors': 61, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07360, params: {'n_neighbors': 61, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10250, params: {'n_neighbors': 62, 'weights': 'uniform'},\n",
       " mean: 0.68379, std: 0.07401, params: {'n_neighbors': 62, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10250, params: {'n_neighbors': 63, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07360, params: {'n_neighbors': 63, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10413, params: {'n_neighbors': 64, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07360, params: {'n_neighbors': 64, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10289, params: {'n_neighbors': 65, 'weights': 'uniform'},\n",
       " mean: 0.68775, std: 0.07487, params: {'n_neighbors': 65, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.10152, params: {'n_neighbors': 66, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07464, params: {'n_neighbors': 66, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.09925, params: {'n_neighbors': 67, 'weights': 'uniform'},\n",
       " mean: 0.68577, std: 0.07464, params: {'n_neighbors': 67, 'weights': 'distance'},\n",
       " mean: 0.64625, std: 0.09907, params: {'n_neighbors': 68, 'weights': 'uniform'},\n",
       " mean: 0.67984, std: 0.07379, params: {'n_neighbors': 68, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.09871, params: {'n_neighbors': 69, 'weights': 'uniform'},\n",
       " mean: 0.67984, std: 0.07413, params: {'n_neighbors': 69, 'weights': 'distance'},\n",
       " mean: 0.65217, std: 0.10126, params: {'n_neighbors': 70, 'weights': 'uniform'},\n",
       " mean: 0.67984, std: 0.07308, params: {'n_neighbors': 70, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.09953, params: {'n_neighbors': 71, 'weights': 'uniform'},\n",
       " mean: 0.67787, std: 0.07574, params: {'n_neighbors': 71, 'weights': 'distance'},\n",
       " mean: 0.64625, std: 0.10202, params: {'n_neighbors': 72, 'weights': 'uniform'},\n",
       " mean: 0.67787, std: 0.07574, params: {'n_neighbors': 72, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.10030, params: {'n_neighbors': 73, 'weights': 'uniform'},\n",
       " mean: 0.67984, std: 0.07567, params: {'n_neighbors': 73, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.10305, params: {'n_neighbors': 74, 'weights': 'uniform'},\n",
       " mean: 0.67589, std: 0.07677, params: {'n_neighbors': 74, 'weights': 'distance'},\n",
       " mean: 0.64625, std: 0.10373, params: {'n_neighbors': 75, 'weights': 'uniform'},\n",
       " mean: 0.67787, std: 0.07420, params: {'n_neighbors': 75, 'weights': 'distance'},\n",
       " mean: 0.64625, std: 0.10262, params: {'n_neighbors': 76, 'weights': 'uniform'},\n",
       " mean: 0.67391, std: 0.07969, params: {'n_neighbors': 76, 'weights': 'distance'},\n",
       " mean: 0.64625, std: 0.10125, params: {'n_neighbors': 77, 'weights': 'uniform'},\n",
       " mean: 0.67391, std: 0.07590, params: {'n_neighbors': 77, 'weights': 'distance'},\n",
       " mean: 0.64427, std: 0.09799, params: {'n_neighbors': 78, 'weights': 'uniform'},\n",
       " mean: 0.67194, std: 0.08168, params: {'n_neighbors': 78, 'weights': 'distance'},\n",
       " mean: 0.65020, std: 0.10344, params: {'n_neighbors': 79, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.08479, params: {'n_neighbors': 79, 'weights': 'distance'},\n",
       " mean: 0.64427, std: 0.10031, params: {'n_neighbors': 80, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09014, params: {'n_neighbors': 80, 'weights': 'distance'},\n",
       " mean: 0.64822, std: 0.10293, params: {'n_neighbors': 81, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09014, params: {'n_neighbors': 81, 'weights': 'distance'},\n",
       " mean: 0.64229, std: 0.10291, params: {'n_neighbors': 82, 'weights': 'uniform'},\n",
       " mean: 0.67194, std: 0.08787, params: {'n_neighbors': 82, 'weights': 'distance'},\n",
       " mean: 0.64427, std: 0.10104, params: {'n_neighbors': 83, 'weights': 'uniform'},\n",
       " mean: 0.67194, std: 0.09256, params: {'n_neighbors': 83, 'weights': 'distance'},\n",
       " mean: 0.64032, std: 0.10077, params: {'n_neighbors': 84, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.09120, params: {'n_neighbors': 84, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.09944, params: {'n_neighbors': 85, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.09162, params: {'n_neighbors': 85, 'weights': 'distance'},\n",
       " mean: 0.63241, std: 0.10094, params: {'n_neighbors': 86, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.09162, params: {'n_neighbors': 86, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.09872, params: {'n_neighbors': 87, 'weights': 'uniform'},\n",
       " mean: 0.66798, std: 0.09020, params: {'n_neighbors': 87, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.09840, params: {'n_neighbors': 88, 'weights': 'uniform'},\n",
       " mean: 0.66798, std: 0.09020, params: {'n_neighbors': 88, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.09700, params: {'n_neighbors': 89, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.08688, params: {'n_neighbors': 89, 'weights': 'distance'},\n",
       " mean: 0.63834, std: 0.09856, params: {'n_neighbors': 90, 'weights': 'uniform'},\n",
       " mean: 0.66996, std: 0.08599, params: {'n_neighbors': 90, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.09944, params: {'n_neighbors': 91, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09043, params: {'n_neighbors': 91, 'weights': 'distance'},\n",
       " mean: 0.63241, std: 0.09935, params: {'n_neighbors': 92, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09169, params: {'n_neighbors': 92, 'weights': 'distance'},\n",
       " mean: 0.62846, std: 0.09718, params: {'n_neighbors': 93, 'weights': 'uniform'},\n",
       " mean: 0.66403, std: 0.09271, params: {'n_neighbors': 93, 'weights': 'distance'},\n",
       " mean: 0.63043, std: 0.09813, params: {'n_neighbors': 94, 'weights': 'uniform'},\n",
       " mean: 0.66403, std: 0.09271, params: {'n_neighbors': 94, 'weights': 'distance'},\n",
       " mean: 0.63043, std: 0.10002, params: {'n_neighbors': 95, 'weights': 'uniform'},\n",
       " mean: 0.66403, std: 0.09271, params: {'n_neighbors': 95, 'weights': 'distance'},\n",
       " mean: 0.63439, std: 0.10109, params: {'n_neighbors': 96, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09169, params: {'n_neighbors': 96, 'weights': 'distance'},\n",
       " mean: 0.63043, std: 0.09875, params: {'n_neighbors': 97, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09169, params: {'n_neighbors': 97, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.10514, params: {'n_neighbors': 98, 'weights': 'uniform'},\n",
       " mean: 0.66798, std: 0.09517, params: {'n_neighbors': 98, 'weights': 'distance'},\n",
       " mean: 0.63636, std: 0.10236, params: {'n_neighbors': 99, 'weights': 'uniform'},\n",
       " mean: 0.66601, std: 0.09169, params: {'n_neighbors': 99, 'weights': 'distance'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "from sklearn import model_selection\n",
    "k = range(1,100)\n",
    "params = {'n_neighbors':k,'weights':['uniform','distance']}\n",
    "\n",
    "kf = cross_validation.KFold(len(x), n_folds = 10)\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=neighbors.KNeighborsClassifier(),\n",
    "    param_grid=params,\n",
    "    cv=kf,\n",
    ")\n",
    "gs.fit(x,y)\n",
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=16, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.  \n",
    "\n",
    "+ Explain your findings\n",
    "+ What were your best parameters?\n",
    "+ What was the best k?\n",
    "+ What was the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best K was 16.\n",
    "HTHe best model used minkowski, leaf size 30, n_neighbords, with uniform weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.  \n",
    "\n",
    "+ Train your model with the optimal `k` you found above \n",
    "+ (don't worry if it changes from time to time - if that is the case use the one that is usually the best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "Best_Model = gs.best_estimator_\n",
    "Best_Model_Predictions = Best_Model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "classification_report = metrics.classification_report(y_true= y,y_pred=Best_Model_Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62, 1.0, 0.76]\n",
      "[0.93, 0.93, 0.93]\n",
      "[0.59, 0.97, 0.73]\n",
      "[0.47, 0.92, 0.62]\n",
      "[1.0, 0.16, 0.28]\n",
      "plotMat: [[0.62, 1.0, 0.76], [0.93, 0.93, 0.93], [0.59, 0.97, 0.73], [0.47, 0.92, 0.62], [1.0, 0.16, 0.28]]\n",
      "support: [66, 40, 67, 272, 413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/averyw/anaconda/lib/python2.7/site-packages/matplotlib/artist.py:233: MatplotlibDeprecationWarning: get_axes has been deprecated in mpl 1.5, please use the\n",
      "axes property.  A removal date has not been set.\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    '''\n",
    "    Heatmap with text in each cell with matplotlib's pyplot\n",
    "    Source: http://stackoverflow.com/a/25074150/395857 \n",
    "    By HYRY\n",
    "    '''\n",
    "    from itertools import izip\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.get_axes()\n",
    "    for p, color, value in izip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: http://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - http://stackoverflow.com/a/16124677/395857 \n",
    "    - http://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on http://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 2)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    print('plotMat: {0}'.format(plotMat))\n",
    "    print('support: {0}'.format(support))\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n",
    "\n",
    "\n",
    "def main():\n",
    "    sampleClassificationReport = \"\"\"             precision    recall  f1-score   support\n",
    "\n",
    "          Acacia       0.62      1.00      0.76        66\n",
    "          Blossom       0.93      0.93      0.93        40\n",
    "          Camellia       0.59      0.97      0.73        67\n",
    "          Daisy       0.47      0.92      0.62       272\n",
    "          Echium       1.00      0.16      0.28       413\n",
    "\n",
    "        avg / total       0.77      0.57      0.49       858\"\"\"\n",
    "\n",
    "\n",
    "    plot_classification_report(sampleClassificationReport)\n",
    "    plt.savefig('test_plot_classif_report.png', dpi=200, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    #cProfile.run('main()') # if you want to do some profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86, 0.8, 0.83]\n",
      "[0.65, 0.77, 0.71]\n",
      "[0.76, 0.77, 0.76]\n",
      "[0.94, 0.59, 0.72]\n",
      "plotMat: [[0.86, 0.8, 0.83], [0.65, 0.77, 0.71], [0.76, 0.77, 0.76], [0.94, 0.59, 0.72]]\n",
      "support: [153, 151, 151, 51]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAEsCAYAAABpB38dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCDshgZkQQgghhJ34f+Yagp9eLjCGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d6240d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_classification_report(classification_report, title='Classification report ', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8.  \n",
    "\n",
    "+ After training your model with that `k`, \n",
    "+ use it to *predict* the class of a neighborhood with `RM = 2`, `PRATIO = 19`, and `LSTAT = 3.5`\n",
    "+ If you are confused, check out the sklearn documentation for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>PRATIO</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RM  PRATIO   RM\n",
       "0  2.0    19.0  3.5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "#x_example = \n",
    "#print x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_example = {'RM': 2.0,'PRATIO':19.0,'LSTAT':3.5}\n",
    "df = pd.DataFrame( [2,19,3.5], index = ['RM','PRATIO','RM']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Level_1'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best_Model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
